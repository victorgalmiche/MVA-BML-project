{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Twisted Diffusion Sampler (TDS)\n",
        "\n",
        "The **Twisted Diffusion Sampler (TDS)** is a Sequential Monte Carlo (SMC) method designed for conditional sampling from diffusion models. It addresses the challenge of sampling from a posterior distribution $p(x_0 | y)$ given a measurement $y$, where the prior $p(x_0)$ is defined by a pre-trained diffusion model.\n",
        "\n",
        "Standard diffusion guidance methods (like classifier guidance) approximate the conditional score $\\nabla_{x_t} \\log p(x_t | y)$. TDS improves upon this by introducing a \"twisting\" function (or auxiliary target) $\\tilde{p}(y|x_t)$ to guide the intermediate proposal distributions in an SMC framework.\n",
        "\n",
        "### Key Steps in Algorithm 1:\n",
        "1.  **Initialization**: Start with particles $x_T$ from the standard normal prior and assign initial weights based on the twisting function.\n",
        "2.  **Resampling**: At each timestep, resample particles based on their importance weights to focus on high-probability regions.\n",
        "3.  **Conditional Score Approximation**: Compute a \"twisted\" score that combines the unconditional diffusion score with the gradient of the twisting function $\\nabla_{x_t} \\log \\tilde{p}(y|x_t)$.\n",
        "4.  **Proposal Step**: Propagate particles to the next timestep $x_{t-1}$ using the twisted score (similar to guidance).\n",
        "5.  **Weight Update**: Update particle weights to account for the discrepancy between the optimal target distribution and the proposal distribution."
      ],
      "metadata": {
        "id": "04ycvS2IG9Lw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe3fb167"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "def twisted_diffusion_sampler(\n",
        "    model,              # The diffusion model (e.g., a UNet that predicts noise or x_0)\n",
        "    scheduler,          # Diffusion scheduler (e.g., DDIM/DDPM scheduler)\n",
        "    y,                  # The observed measurement (y)\n",
        "    twisting_func,      # Function that computes log_prob of y given x_t: log_p(y|x_t)\n",
        "    num_particles=2,    # K: Number of particles\n",
        "    num_steps=100,        # T: Number of timesteps\n",
        "    shape=(3, 256, 256),# Shape of x\n",
        "    device='cuda'\n",
        "):\n",
        "    \"\"\"\n",
        "    Implementation of Algorithm 1: Twisted Diffusion Sampler (TDS)\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Initialization\n",
        "    # x_T ~ p(x_T) (Standard Normal)\n",
        "    # shape needs to include batch size K\n",
        "    x_t = torch.randn((num_particles, *shape), device=device)\n",
        "\n",
        "    # Initial weights\n",
        "    # w_k = p_tilde(y | x_T)\n",
        "    with torch.no_grad():\n",
        "        log_weights = twisting_func(x_t, y, num_steps)\n",
        "        weights = torch.softmax(log_weights, dim=0)\n",
        "\n",
        "    # Time loop T-1 to 0\n",
        "    # Note: scheduler timesteps usually go from T-1 down to 0\n",
        "    scheduler.set_timesteps(num_steps)\n",
        "\n",
        "    for i, t in enumerate(scheduler.timesteps):\n",
        "        # 2. Resampling\n",
        "        # Resample indices based on weights\n",
        "        indices = torch.multinomial(weights, num_particles, replacement=True)\n",
        "        x_t = x_t[indices]\n",
        "\n",
        "        # Current twisting value for the resampled particles (log_p_tilde_{t+1})\n",
        "        # We re-evaluate or carry over. For simplicity, re-evaluating here or caching is needed.\n",
        "        # Since x_t changed, we should technically use the weights' associated probs,\n",
        "        # but let's re-calculate gradients typically needed for the score.\n",
        "\n",
        "        # Enable grad for the score approximation step\n",
        "        x_t = x_t.detach().requires_grad_(True)\n",
        "\n",
        "        # 3. Conditional Score Approximation\n",
        "        # Estimate x_0 (x_hat_theta)\n",
        "        model_output = model(x_t, t) # Assuming model outputs noise or x_0\n",
        "\n",
        "        # Convert model output to x_0 prediction using scheduler\n",
        "        # (This depends on specific scheduler API, here is a generic placeholder)\n",
        "        # For DDIM/DDPM, we usually get prev_sample, but we need x_0 for the formula.\n",
        "        # Let's assume a function `get_x0_from_noise` exists or scheduler provides it.\n",
        "        # step_output = scheduler.step(model_output, t, x_t)\n",
        "        # x_0_hat = step_output.pred_original_sample\n",
        "\n",
        "        # Simplified: let's assume we calculate the score update directly:\n",
        "        # s_tilde = (x_0_hat - x_t) / sigma^2 + grad(log_p_tilde_{t+1})\n",
        "\n",
        "        # Calculate log twisting function for gradients\n",
        "        log_twisting = twisting_func(x_t, y, t)\n",
        "        grad_log_twisting = torch.autograd.grad(log_twisting.sum(), x_t)[0]\n",
        "\n",
        "        # Standard diffusion score (unconditional)\n",
        "        # score_uncond = (x_0_hat - x_t) / sigma_t^2\n",
        "        # Often schedulers give us the mean of p(x_{t-1} | x_t).\n",
        "        # Let's use the \"Proposal\" logic defined in the algorithm:\n",
        "        # x_{t-1} ~ N(x_t + sigma^2 * s_tilde, ...)\n",
        "\n",
        "        # 4. Proposal Step (Transition)\n",
        "        # We combine the unconditional transition with the twisting gradient\n",
        "        # This is equivalent to Classifier Guidance usually.\n",
        "        with torch.no_grad():\n",
        "            # Standard step to get parameters for p(x_{t-1} | x_t)\n",
        "            # This usually returns x_{t-1} mean and variance.\n",
        "            step_output = scheduler.step(model_output, t, x_t)\n",
        "\n",
        "            prev_sample_mean = step_output.prev_sample # This usually includes the drift towards x_0\n",
        "\n",
        "            # Adjust mean with twisting gradient\n",
        "            # The scale factor depends on the variance schedule (beta_t or sigma_t)\n",
        "            # Typically: new_mean = old_mean + variance * gradient\n",
        "            variance = scheduler._get_variance(t) if hasattr(scheduler, '_get_variance') else 1.0 # Placeholder\n",
        "\n",
        "            proposal_mean = prev_sample_mean + variance * grad_log_twisting\n",
        "\n",
        "            # Sample x_{t-1}\n",
        "            noise = torch.randn_like(x_t)\n",
        "            x_prev = proposal_mean + torch.sqrt(torch.tensor(variance)) * noise\n",
        "\n",
        "            # 5. Weight Update\n",
        "            # w = p(x_t | x_{t+1}) * p_tilde_t / [ p_tilde(x_t | x_{t+1}, y) * p_tilde_{t+1} ]\n",
        "            # In many Twisted implementations, if the proposal is optimal, weights are uniform.\n",
        "            # Otherwise, we compute the importance weights.\n",
        "\n",
        "            # Calculate new twisting func value\n",
        "            log_p_tilde_t = twisting_func(x_prev, y, t-1 if t>0 else 0)\n",
        "\n",
        "            # Update weights (simplified log domain update)\n",
        "            # For exact TDS, we need the transition probability densities.\n",
        "            # Often approximated or set to 1 if proposal is very good.\n",
        "            log_weights = log_p_tilde_t # + transition terms correction\n",
        "            weights = torch.softmax(log_weights, dim=0)\n",
        "\n",
        "            x_t = x_prev\n",
        "\n",
        "    return x_t"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "594b2eb3",
        "outputId": "883e233b-1b27-4374-a68e-b12cf5ee8ad0"
      },
      "source": [
        "from diffusers import DDPMScheduler, UNet2DModel\n",
        "\n",
        "# Chargement d'un modèle adapté aux images (ex: entraîné sur des visages ou paysages)\n",
        "model = UNet2DModel.from_pretrained(\"google/ddpm-celebahq-256\")\n",
        "scheduler = DDPMScheduler.from_pretrained(\"google/ddpm-celebahq-256\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n",
            "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py:202: UserWarning: The `local_dir_use_symlinks` argument is deprecated and ignored in `hf_hub_download`. Downloading to a local directory does not use symlinks anymore.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "An error occurred while trying to fetch google/ddpm-celebahq-256: google/ddpm-celebahq-256 does not appear to have a file named diffusion_pytorch_model.safetensors.\n",
            "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
          ]
        }
      ]
    }
  ]
}